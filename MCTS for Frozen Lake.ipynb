{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register  # https://github.com/openai/gym/blob/master/gym/envs/registration.py\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "MY_ENV_NAME='FrozenLakeNonskid4x4-v3'\n",
    "\n",
    "# 0 = left; 1 = down; 2 = right;  3 = up\n",
    "# reward of 1 if success, no reward otherwise\n",
    "# done = True even if not success due to timestep limit\n",
    "\n",
    "register(\n",
    "    id=MY_ENV_NAME,\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    reward_threshold=0.78)\n",
    "\n",
    "env = gym.make(MY_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    each state will be represented as a State object. we do this \n",
    "    because we want to track (a) total rewards and (b) play count.\n",
    "    \n",
    "    we need to track rewards and play count in order to calculate\n",
    "    UCB score; we want to calculate UCB score in order to select \n",
    "    the most promising action.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.q = 0  # win count\n",
    "        self.n = 0  # play count\n",
    "\n",
    "def _find_valid_actions(s1, env):\n",
    "    \"\"\"return a list of valid (action, state2) pairs for a given state.\n",
    "    \"\"\"\n",
    "    action_state = []\n",
    "    for a in range(3):  # 4 possible actions at any grid\n",
    "        s2, _, _, _ = env.step(a)\n",
    "        \n",
    "        # scenario 1: action does not lead to a new state, so ignore\n",
    "        if s2 == s1:\n",
    "            pass\n",
    "        # scenario 2: action leads to a new state, so keep\n",
    "        else:\n",
    "            action_state.append((a, s2))\n",
    "        env.s = s1  # return env to original state\n",
    "    return action_state\n",
    "        \n",
    "def select_action(s, memory, env):\n",
    "    \"\"\"\n",
    "    return action corresponding to the child with highest UCB score.\n",
    "    \n",
    "    s is the current state; memory is the dict containing all states\n",
    "    we've seen in the current rollout; and env is the current \n",
    "    environment.\n",
    "    \"\"\"\n",
    "    # step 1: find all actions that would lead to a valid state\n",
    "    action_state = _find_valid_actions(s, env)\n",
    "    \n",
    "    # step 2a: if we havent seen that child before, then go with it\n",
    "    for ap in action_state:\n",
    "        a = ap[0]\n",
    "        s2 = ap[1]\n",
    "        if s2 in memory.keys():\n",
    "            pass\n",
    "        else:\n",
    "            return a  # this action leads to a state that we havent seen before\n",
    "        \n",
    "    # step 2b: if we've visited all possible s2, return action that would lead to s2 with the highest UCB\n",
    "    action = None\n",
    "    max_ucb = 0\n",
    "    for ap in action_state:\n",
    "        s2 = memory[ap[1]]\n",
    "        s1 = memory[s]\n",
    "        score = calculate_ucb(s1, s2)\n",
    "        if score > max_ucb:\n",
    "            action = ap[0]\n",
    "            max_ucb = score  # new score to beat\n",
    "    if action is not None:\n",
    "        return action\n",
    "    # if there isnt a clear candidate, randomly select from all possible action/state pairs\n",
    "    else:\n",
    "        actions = [ap[0] for ap in action_state]\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "def calculate_ucb(s1, s2):\n",
    "    c_param = .14\n",
    "    term_1 = (s2.q / s2.n)  # can be interpreted as the value estimate\n",
    "    term_2 = c_param * np.sqrt((2 * np.log(s1.n) / s2.n))\n",
    "    return  term_1 + term_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = 14\n",
    "n_rollouts = 10\n",
    "max_steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace [14, 13, 12]\n",
      "completed rollout 0 with reward 0.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 1 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 2 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 3 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 4 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 5 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 6 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 7 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 8 with reward 1.0\n",
      "--------------------\n",
      "trace [14, 15]\n",
      "completed rollout 9 with reward 1.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "memory = {starting_state: Node()}  # track states that we've seen before since we cant \"color\" environment\n",
    "\n",
    "for rollout in range(n_rollouts):    \n",
    "\n",
    "    # starting_state is the root node; ultimately we want to know which action we should take from the root node\n",
    "    trace = [starting_state]\n",
    "    env.s = starting_state  # manually set env to the state we want\n",
    "    s = starting_state  # s will be updated as we play out each rollout\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        a = select_action(s, memory, env)  # select actions based on UCB scores\n",
    "        s, r, done, _ = env.step(a)\n",
    "        steps += 1\n",
    "\n",
    "        # if we've seen state s before, we dont need to do anything\n",
    "        if s in memory:\n",
    "            pass\n",
    "        # if we havent seen state s before, we update memory and continue\n",
    "        else:\n",
    "            memory[s] = Node()\n",
    "\n",
    "        # add action to trace so we can backpropagate from terminal through root\n",
    "        trace.append(s)\n",
    "\n",
    "\n",
    "    # now that we're done with the rollout...\n",
    "    print(\"trace\", trace)\n",
    "    print(\"completed rollout\", rollout, \"with reward\", r)\n",
    "\n",
    "    # backpropagate the nodes in the trajectory\n",
    "    for t in set(trace):\n",
    "        node = memory[t]  # retrieve node for corresponding state\n",
    "        node.q += r  # update reward\n",
    "        node.n += 1  # update games played\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> best move 2\n",
      ">> new state 15\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 0 = left; 1 = down; 2 = right;  3 = up\n",
    "# now that we're done sampling, lets find the best move (it's the one that results in the child with the most plays)\n",
    "best = select_action(starting_state, memory, env)\n",
    "print(\">> best move\", best)\n",
    "s2, _, _, _ = env.step(best)\n",
    "print(\">> new state\", s2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
